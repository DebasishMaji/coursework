# Linear Regression

   Model
    
    yi = Œ≤0 + Œ≤1xi + Œµi
   
    Œµi ‚àº N (0, œÉ)
   
   The estimates Œ≤ÀÜ0 and Œ≤ÀÜ1 are chosen to minimize the residual sum of squares (RSS): 
        
        RSS = summation over i belongs to (1, n) (yi ‚àí yÀÜi)**2
        
            = summation over i belongs to (1, n) (yi ‚àí Œ≤0 ‚àí Œ≤1xi)**2
        
   #### Estimates Œ≤ÀÜ0 and Œ≤ÀÜ1
   
   A little calculus shows that the minimizers of the RSS are:
   
        Œ≤ÀÜ1 = (summation over i belongs to (1, n) (xi ‚àí x')(yi ‚àí y)) / (summation over i belongs (1, n) (xi ‚àí x)**2)
        
        Œ≤ÀÜ0 = y ‚àí Œ≤ÀÜ

  #### Assesing the accuracy of Œ≤ÀÜ0 and Œ≤ÀÜ1
  
  The Standard Errors for the parameters are:
    
        SE(Œ≤ÀÜ0)**2 = œÉ**2[1/n + x'**2/(summation over i belongs to (1, n)(xi - x')**2)
        
        SE(Œ≤ÀÜ1)**2 = œÉ**2 / (summation over i belongs to (1, n) (xi ‚àí x)**2)

 The 95% confidence intervals:
    
    Œ≤ÀÜ0 ¬± 2 ¬∑ SE(Œ≤ÀÜ0)

    Œ≤ÀÜ1 ¬± 2 ¬∑ SE(Œ≤ÀÜ1)
    
  Calculations depend on the model

    
  #### Hypothesis test

   H0 : there is no relationship between X and Y
   Ha : There is some relationship between X and Y
   
       H0: Œ≤1 = 0
       Ha: Œ≤1 != 0
    
   Test statistic:
   
   t = (Œ≤ÀÜ1‚àí0) / SE(Œ≤ÀÜ1)

   Under the null hypothesis (special case of the model), this has a t-distribution
with n ‚àí 2 degrees of freedom.

  #### Interpreting the hypothesis test
  
   * If we reject the null hypothesis, can we assume there is an
exact linear relationship?

         No. A quadratic relationship may be a better fit, for example.
There is evidence of linear association.

   * If we don‚Äôt reject the null hypothesis, can we assume there is
no relationship between X and Y ?
    
         No. This test is only powerful against certain monotone alternatives. There could be more complex non-linear relationships.
         
   #### Multiple linear regression
   
   
        Y = Œ≤0 + Œ≤1X1 + ¬∑ ¬∑ ¬∑ + Œ≤pXp + Œµ    where Œµ ‚àº N (0, œÉ) 
        
        or, in matrix notation:
        
        Ey = XŒ≤, where y = (y1, . . . , yn)T, Œ≤ = (Œ≤0, . . . , Œ≤p)T and X is our usual data matrix with an extra column of ones on the left to account for the intercept.
        
  #### Multiple linear regression can answer several
  
  * Is at least one of the variables Xi useful for predicting the
outcome Y ?

   * Which subset of the predictors is most important?
   
   * How good is a linear model for these data?
   
   * Given a set of predictor values, what is a likely value for Y , and how accurate is this prediction?
   
   
   #### The estimates Œ≤ÀÜ
   
   Our goal again is to minimize the RSS:
   
        RSS = summation over i belongs to (1, n) (yi ‚àí yÀÜi)**2
        
            = summation over i belongs to (1, n) (yi ‚àí Œ≤0 ‚àí Œ≤1xi,1 ‚àí ¬∑ ¬∑ ¬∑ ‚àí Œ≤pxi,p)**2
    
   One can show that this is minimized by the vector Œ≤ÀÜ:
      
        Œ≤ÀÜ = (XT X)‚àí1XTy
        
   We also write RSS for the minimized sum of squares.
   
  #### Which variables are important?
  
  Consider the hypothesis:
  
    H0 : The last q predictors have no relation with Y 
    
    H0 : Œ≤p‚àíq+1 = Œ≤p‚àíq+2 = ¬∑ ¬∑ ¬∑ = Œ≤p = 0
    
   Let RSS0 be the residual sum of squares for the model which
excludes these variables.

    The F-statistic is defined by:
    
            F = ((RSS0 ‚àí RSS)/q)/(RSS/(n ‚àí p ‚àí 1))
      
   Under the null hypothesis, this has an F-distribution
   
   Example: If q = p, we test whether any of the variables is important
   
   The t-statistic associated to the ith predictor is the square root of the F-statistic for the null hypothesis which sets only Œ≤i = 0
   
   A low p-value indicates that the predictor is important (it adds something to the model not explained by other variables)
   
   P-hacking warning: If there are many predictors, even under the null hypothesis, some of the t-tests will have low p-values
   
   #### How many variables are important?
   
   When we select a subset of the predictors, we have 2**p choices
   
   A way to simplify the choice is to define a range of models with an increasing number of variables, then select the best
   
   *  Forward selection: Starting from a null model, include variables one at a time, minimizing the RSS at each step
   
   * Backward selection: Starting from the full model, eliminate variables one at a time, choosing the one with the largest p-value at each step
   
   * Mixed selection: Starting from a null model, include variables one at a time, minimizing the RSS at each step. If the p-value for some variable goes beyond a threshold, eliminate that variable
   
   Choosing model this way is a form of tuning. P-hacking: hypothesis tests, confidence intervals not valid after selection! 
   
   #### How good is the fit?
   
   To assess the fit, we focus on the residuals
   
   * The RSS always decreases as we add more variables
   
   * The residual standard error (RSE) corrects this
   
         RSE = sqrt(RSS/(n ‚àí p ‚àí 1))
         
   * Visualizing the residuals can reveal phenomena that are not accounted for by the model
   
   
   #### Dealing with categorical or qualitative predictors
   
   Example: Credit dataset
   
   * gender: male, female
   * student: student or not
   * status: married, single, divorced
   * ethnicity: African, American, Asian, Caucasia
   
   For each qualitative predictor, e.g. ethnicity:
   
   * Choose a baseline category, e.g. African American
   
   * For every other category, define a new predictor:
    
        * XAsian is 1 if the person is Asian and 0 otherwise
        * XCaucasian is 1 if the person is Caucasian and 0 otherwise
        
   The model will be: 
   
    Y = Œ≤0 + Œ≤1X1 + ¬∑ ¬∑ ¬∑+ Œ≤7X7 + Œ≤Asian XAsian + Œ≤Caucasian XCaucasian + Œµ.
    
    
   Œ≤Asian is the relative effect on balance for being Asian compared to the baseline category
   
   * The model fit and predictions are independent of the choice of the baseline category

   * However, hypothesis tests derived from these variables are affected by the choice
    
        * Solution: To check whether ethnicity is important, use an F-test for the hypothesis Œ≤Asian = Œ≤Caucasian = 0. This does not depend on the coding
   
   * Other ways to encode qualitative predictors produce the same fit ÀÜf, but the coefficients have different interpretations
   
   #### How good is the fit?
   
   To assess the fit, we focus on the residuals.
   
   * R2 = Corr(Y, YÀÜ ), always increases as we add more variables
   
   * The residual standard error (RSE) does not always improve with more predictors
   
         RSE = sqrt(RSS/(n ‚àí p ‚àí 1))

   * Visualizing the residuals can reveal phenomena that are not accounted for by the model
   
   #### Potential issues in linear regression
   
   * Interactions between predictors
   * Non-linear relationships
   * Correlation of error terms
   * Non-constant variance of error (heteroskedasticity)
   * Outliers
   * High leverage points
   * Co-linearity
   
   #### Interactions between predictors
   
   Linear regression has an additive assumption:
   
     sales = Œ≤0 + Œ≤1 √ó tv + Œ≤2 √ó radio + Œµ
     
   i.e. An increase of $100 dollars in TV ads causes a fixed increase in sales, regardless of how much you spend on radio ads
   
   If we visualize the residuals, it is clear that this is false
   
   One way to deal with this is to include multiplicative variables in the model:
   
    sales = Œ≤0 + Œ≤1 √ó tv + Œ≤2 √ó radio + Œ≤3 √ó (tv ¬∑ radio) + Œµ
    
   The interaction variable is high when both tv and radio are high.
   
   * Non-linearities
   
   A scatter plot between a predictor and the response may reveal a non-linear relationship.
   
   * Solution: include polynomial terms in the model.
   
         MPG = Œ≤0 + Œ≤1 √ó horsepower + Œµ
         
         MPG = Œ≤0 + Œ≤1 √ó horsepower + Œ≤2 √ó horsepower2 + Œµ
         
         MPG = Œ≤0 + Œ≤1 √ó horsepower + Œ≤2 √ó horsepower2 + Œ≤3 √ó horsepower3 + Œµ
         
         MPG = Œ≤0 + Œ≤1 √ó horsepower + Œ≤2 √ó horsepower2 + Œ≤3 √ó horsepower3 + . . . + Œµ
         
   In 2 or 3 dimensions, this is easy to visualize. What do we do when we have too many predictors?
   
   Plot the residuals against the response and look for a pattern
   
   #### Correlation of error terms
   
   We assumed that the errors for each sample are independent:
   
        yi = f(xi) + Œµi; Œµi ‚àº N (0, œÉ) 
        
   ##### What if this breaks down?
   
   The main effect is that this invalidates any assertions about Standard Errors, confidence intervals, and hypothesis tests
   
   Example: Suppose that by accident, we double the data (we use each sample twice). Then, the standard errors would be artificially smaller by a factor of ‚àö2
   
   When could this happen in real life:
   
   * Time series: Each sample corresponds to a different point in time. The errors for samples that are close in time are correlated.
   
   * Spatial data: Each sample corresponds to a different location in space
   
   * Study on predicting height from weight at birth. Suppose some of the subjects in the study are in the same family, their shared environment could make them deviate from f(x) in similar ways
   
   #### Non-constant variance of error (heteroskedasticity)
   
   The variance of the error depends on the input.
   
   To diagnose this, we can plot residuals vs. fitted values:
   
   * Solution: If the trend in variance is relatively simple, we can
transform the response using a logarithm, for example

   #### Outliers
   
   Outliers are points with very high errors.
   
   While they may not affect the fit, they might affect our assessment of model quality
   
   Possible solutions:
   
   * If we believe an outlier is due to an error in data collection, we can remove it

   * An outlier might be evidence of a missing predictor, or the need to specify a more complex model
   
  #### High leverage points
  
  Some samples with extreme inputs have an out sized effect on Œ≤ÀÜ.
  
  This can be measured with the leverage statistic or self influence
  
        hii = ‚àÇyÀÜi/‚àÇyi = (X(XT X)‚àí1XT)i,i ‚àà [1/n, 1].
        
  #### Studentized residuals
  
  * The residual ÀÜi = yi ‚àí yÀÜi is an estimate for the noise i
  
  * The standard error of ŒµÀÜi is œÉ ‚àö(1 ‚àí hii)
  
  * A studentized residual is ŒµÀÜi divided by its standard error
  
  * When model is correct, it follows a Student-t distribution with n ‚àí p ‚àí 2 degrees of freedom
  
  #### Collinearity
  
  Two predictors are collinear if one explains the other well:
  
    limit = a √ó rating + b
    
  i.e. they contain the same information
  
  Problem: The coefficients become unidentifiable. Consider the extreme case of using two identical predictors limit:
  
      balance = Œ≤0 + Œ≤1 √ó limit + Œ≤2 √ó limit
      
      balance = Œ≤0 + Œ≤1 √ó limit + Œ≤2 √ó limit = Œ≤0 + (Œ≤1 + 100) √ó limit + (Œ≤2 ‚àí 100) √ó limit
  
  The fit (Œ≤ÀÜ0, Œ≤ÀÜ1, Œ≤ÀÜ2) is just as good as (Œ≤ÀÜ0, Œ≤ÀÜ1 + 100, Œ≤ÀÜ2 ‚àí 100)
  
  If 2 variables are collinear, we can easily diagnose this using their correlation
  
  A group of q variables is multi linear if these variables ‚Äúcontain less information‚Äù than q independent variables. Pairwise correlations may not reveal multi linear variables
  
  The Variance Inflation Factor (VIF) measures how necessary a variable is, or how predictable it is given the other variables:
  
    V IF(Œ≤ÀÜj ) = 1/ (1 ‚àí R2Xj |X‚àíj)
    
   where R2Xj |X‚àíj is the R2 statistic for Multiple Linear regression of the predictor Xj onto the remaining predictors
   
   #### Comparing Linear Regression to K-nearest neighbors
   
   Linear regression: prototypical parametric method. Inference
   
   KNN regression: prototypical nonparametric method. Inference?
   
        fÀÜ(x) = 1/K (summation over i ‚àà NK(x) yi)
        
   Long story short:
   
   * KNN is only better when the function f is not linear
   
   * When n is not much larger than p, even if f is nonlinear, Linear Regression can outperform KNN
   
   #### When there are more predictors than observations, Linear Regression dominates

   When p  n, each sample has no nearest neighbors, this is known as the curse of dimensionality
   
   The variance of KNN regression is very large